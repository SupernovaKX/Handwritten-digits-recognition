{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9151f959",
   "metadata": {},
   "source": [
    "### Using NN to recognize handwritten digits\n",
    "\n",
    "\n",
    "We will be building a neural network to classify images to their respective digits.  \n",
    "\n",
    "We will build and train a model on the classic **MNIST Handwritten Digits** dataset. Each grayscale image is a $28 \\times 28$ matrix/tensor that looks like so:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"500\" />\n",
    "\n",
    "MNIST is a classification problem and the task is to take in an input image and classify them into one of ten buckets: the digits from $0$ to $9$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e247a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T09:32:04.973799Z",
     "start_time": "2024-11-20T09:32:02.640340Z"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL FIRST\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jinja2 import optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda667dc",
   "metadata": {},
   "source": [
    "### Loading an external dataset\n",
    "\n",
    "The cell below imports the MNIST dataset, which is already pre-split into train and test sets.  \n",
    "\n",
    "The download takes approximately 63MB of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839bbcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce62735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T09:32:33.803854Z",
     "start_time": "2024-11-20T09:32:04.975808Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL â€“ THIS DOWNLOADS THE MNIST DATASET\n",
    "# RUN THIS CELL BEFORE YOU RUN THE REST OF THE CELLS BELOW\n",
    "# Install torchvision\n",
    "from torchvision import datasets\n",
    "\n",
    "# This downloads the MNIST datasets ~63MB\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, download=True)\n",
    "mnist_test  = datasets.MNIST(\"./\", train=False, download=True)\n",
    "\n",
    "x_train = mnist_train.data.reshape(-1, 784) / 255\n",
    "y_train = mnist_train.targets\n",
    "    \n",
    "x_test = mnist_test.data.reshape(-1, 784) / 255\n",
    "y_test = mnist_test.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e092f6c4",
   "metadata": {},
   "source": [
    "### Define the model architechure and implement the forward pass\n",
    "Create a 3-layer network in the `__init__` method of the model `DigitNet`.  \n",
    "These layers are all `Linear` layers and should correspond to the following the architecture:\n",
    "\n",
    "<img src=\"img_linear_nn.png\" width=\"600\">\n",
    "\n",
    "In our data, a given image $x$ has been flattened from a 28x28 image to a 784-length array.\n",
    "\n",
    "After initializing the layers, stitch them together in the `forward` method. The network should look like so:\n",
    "\n",
    "$$x \\rightarrow \\text{Linear(512)} \\rightarrow \\text{ReLU} \\rightarrow \\text{Linear(128)} \\rightarrow \\text{ReLU} \\rightarrow \\text{Linear(10)} \\rightarrow \\text{Softmax} \\rightarrow \\hat{y}$$\n",
    "\n",
    "**Softmax Layer**: The final softmax activation is commonly used for classification tasks, as it will normalizes the results into a vector of values that follows a probability distribution whose total sums up to 1. The output values are between the range [0,1] which is nice because we are able to avoid binary classification and accommodate as many classes or dimensions in our neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d04f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T09:32:33.812353Z",
     "start_time": "2024-11-20T09:32:33.805440Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class DigitNet(nn.Module):\n",
    "    def __init__(self, input_dimensions, num_classes): # set the arguments you'd need\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        - create the 3 layers (and a ReLU layer) using the torch.nn layers API\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(input_dimensions, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x, softmax_output=False):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the network.\n",
    "        \n",
    "        PARAMS:\n",
    "            x : the input tensor (batch size is the entire dataset)\n",
    "            \n",
    "        RETURNS\n",
    "            the output of the entire 3-layer model\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"        \n",
    "        - pass the inputs through the sequence of layers\n",
    "        - run the final output through the Softmax function on the right dimension!\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))  # First layer + ReLU\n",
    "        x = F.relu(self.fc2(x))  # Second layer + ReLU\n",
    "        x = self.fc3(x)          # Third layer (logits)\n",
    "        if softmax_output:       # Apply Softmax if specified\n",
    "            x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d356b9ad",
   "metadata": {},
   "source": [
    "### 3.2 Training Loop\n",
    "\n",
    "As demonstrated in Section 3.2, implement the function `train_model` that performs the following for every epoch/iteration:\n",
    "\n",
    "1. set the optimizer's gradients to zero\n",
    "2. forward pass\n",
    "3. calculate the loss\n",
    "4. backpropagate using the loss\n",
    "5. take an optimzer step to update weights\n",
    "\n",
    "This time, use the Adam optimiser to train the network.  \n",
    "Use Cross-Entropy Loss, since we are performing a classification.  \n",
    "Train for 20 epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab3632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T09:35:01.234624Z",
     "start_time": "2024-11-20T09:32:33.813363Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(x_train, y_train, epochs=20):\n",
    "    \"\"\"\n",
    "    Trains the model for 20 epochs/iterations\n",
    "    \n",
    "    PARAMS:\n",
    "        x_train : a tensor of training features of shape (60000, 784)\n",
    "        y_train : a tensor of training labels of shape (60000, 1)\n",
    "        epochs  : number of epochs, default of 20\n",
    "        \n",
    "    RETURNS:\n",
    "        the final model \n",
    "    \"\"\"\n",
    "    model = DigitNet(784, 10)\n",
    "    optimiser = optim.Adam(model.parameters(), lr=1e-3) # use Adam\n",
    "    loss_fn = nn.CrossEntropyLoss()   # use cross-entropy loss\n",
    "\n",
    "    dataset = TensorDataset(x_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            optimiser.zero_grad()\n",
    "            y_pred = model(batch_x)  # No Softmax here\n",
    "            loss = loss_fn(y_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "                \n",
    "digit_model = train_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fdee35",
   "metadata": {},
   "source": [
    "## Explore the model\n",
    "\n",
    "Now that we have trained the model, let us run some predictions on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83aa93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T09:35:01.373014Z",
     "start_time": "2024-11-20T09:35:01.237636Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is a demonstration: You can use this cell for exploring your trained model\n",
    "digit_model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx in range(3):\n",
    "        scores = digit_model(x_test[idx:idx+1], softmax_output=True)\n",
    "        print(scores)\n",
    "        _, predictions = torch.max(scores, 1)\n",
    "        print(\"true label:\", y_test[idx].item())\n",
    "        print(\"pred label:\", predictions[0].item())\n",
    "        \n",
    "        plt.imshow(x_test[idx].numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc94586",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Now that we have trained the model, we should evaluate it using our test set.  \n",
    "We will be using the accuracy (whether or not the model predicted the correct label) to measure the model performance.  \n",
    "\n",
    "Since our model takes in a (n x 784) tensor and returns a (n x 10) tensor of probability scores for each of the 10 classes, we need to convert the probability scores into the actual predictions by taking the index of the maximum probability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5684246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T09:35:01.440848Z",
     "start_time": "2024-11-20T09:35:01.375030Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(scores, labels):\n",
    "    \"\"\"\n",
    "    helper function that returns accuracy of model (out of 100%)\n",
    "    PARAMS:\n",
    "        scores : the raw softmax scores of the network\n",
    "        label : the ground truth labels\n",
    "        \n",
    "    RETURNS:\n",
    "        accuracy out of 100%\n",
    "    \"\"\"\n",
    "    # Get the predicted class by finding the index of the maximum score\n",
    "    predictions = torch.argmax(scores, dim=1)\n",
    "    \n",
    "    # Compare predictions with the ground truth labels\n",
    "    correct_predictions = (predictions == labels).float()\n",
    "    \n",
    "    # Compute accuracy as a percentage\n",
    "    accuracy = correct_predictions.mean().item() * 100\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "scores = digit_model(x_test) # n x 10 tensor\n",
    "get_accuracy(scores, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
